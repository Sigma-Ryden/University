Сначала о самом проекте:
Это простая но в тоже время расширяемая библиотека для машинного обучения позволяет обучать нейронные сети разных типов
любой сложности, а также пользоваться приемуществами регрессионного анализа. Но хватит ли компъютерных ресурсов
для достижения желаемой цели?
Под расширяемостью подразумевается удачно спроектированная архитектура библиотеки которая далась не сразу.
Главною целью создания была мотивация сделать не просто библиотеку как аналог другим, а возможность
кастомизации не изменяя код само библиотеки без потери производительности. В этом и кроется вся суть гибкости.
Мы не просто можем использовать готовые компоненты которые предостявляет библиотека, но и создавать свои и с легкостью
их интегрировать в существующие.
Простой пример: любая нейросеть при обучении всё возможное время тратит на матричные вычисления, а именно
умножение матриц и векторов. Профилирование это подтверждает, более чем достаточно написать 3-х слойную полносвязную
нейросеть среднего размера примерно с 1500 нейронами и почувствовать насколько медленно продвигается процесс обучения.
Поэтому в библиотеке предоставляется механизм создания своих типов векторов и матриц, а также общего класса для
связывания операций над ними. И это лишь малая часть того, что можно кастомизировать.

Поверхностное описание ключевых внутренностей библиотеки:
Как и любая другая обьемная библиотека - имеет опорные классы для использования. В данном случае все вертится вокруг
класса TrixyNet. Хоть архитектурно это не является основой для других классов нейронных сетей - зато это удобный псевдоним
для их описания. На данным момент поддерживается 2 типа нейронных сетей - это Unified и FeeedForward.
Первый тип является общим для всех других типов, даже тех, которые ещё не появились. Она довольно гибкая, так как допускает
разновидность топологии, которая может включать в себя разные виды слоев. Второй тип - полносвязная нейросеть,
которая является первым прототипом нейросети в библиотеке. По своей сущности является частным случаем первого типа нейросети.
Имеется ввиду, что каждый тип специфичен, и имеет свои интерфейсы взаимодействия, хотя от части они похожи.
При этом в силу того, что пользователю разрешена любая катомизация - библиотека ограждает от неправильной интеграции
этих компонентов, предоставляя специальные классы Require. Бонусом является то, что потенциальному пользователю
не прийдется работать с ними на прямую, за всем позаботится библиотека.
Главной фичей является открытость интерфейса нейросети - мы можем создать её динамически и использовать её топологию со
своими алгоритмами обучения. Но чтобы не нарушить роботу нейросети используются специальные Locker типы. В ним мы оборачиваем
наши составные типы, которые подвергаются изменениям из вне. На самом низком уровне локирующие типы разрешают использовать
обьект для чтения и записи, но запрещают любую реалокацию памяти для этого обьекта. Например контейнер с 8 элементами можно
использовать как обычный контейнер, но ограничены любые операции удаления, вставки, изменения размера этого контейнера и не только.
Появилась такая нужда в локирующих типах только потому, что любое неправильное изменение тополлогии нейросети приведет к плохим последствиям.
Всё потому, что превыше всего ставилась производительность, а лишние проверки сильно бы замедляли и так столь ёмкий процесс обучения.
Процесс кастомизации происходит с помощью специального ядра типов, которое передаётся типу нейросети, который в свою очередь
проверяет валидность этих типов. Почему нельзя проверять эти типы сразу в ядре? Ответ - можно, но теперь для каждого нового
ядра нужно постоянно прописывать валидацию, вместо того, чтобы предоставить это нейросети, которая сама знает какие типы её нужны.

Второстепенные но не менее важные компоненты:
Чтобы нейросеть быстрее обучалась - важно правильно подобрать не только подходящую топологию под решаемую задачу, но и алгоритм
оптимизации. Эта важная составляющая сильно влияет на скорость обучения нейросети. Ведь предоставляемые пакеты данных могут
иметь сильный шум и разброс, а внутрення топология нейросети плохие начальные веса и смещения. Так же важно подобрать подходящие
функции активации - они предают нейросети рельефности (если смотреть на неё как на сложную многомерную математическую функцию -
которой она на самом деле и является). Помимо функций активации в нейросети должен присутствовать своя нормализирующая функция.
Обычно она располагается на самом последнем слое всеё неросети, но никто не запрещает использовать её в середине или даже в начале.
Но чем сложнее функция и её производная тем дольше обучается нейросеть. Но вопрос как нейросеть знает как обучаться? На помощь
приходит самый известный метод обучения нейросетей - градиентный спуск. Оптимизаторы о которых упоминалось ранее - стабилизируют
градиентный спуск и делают его более предсказуемым. Для того, чтобы нейросеть обучалась важнее всего иметь предикат, на основе
которого будет обучаться нейросеть. В случае с классическими нейросетями - это функция ошибки. Минимизация ошибки на основе
градиентного спуска и есть обучение. Если нейросеть подкрепляющего типа, то за основу берутся другие алгоритмы обучения, так как
там отсутствуют пакеты данных. Топология нейросети имеет большой вес на скорость сходимости алгоритма, но плохо проинициализированная
нейросеть будет долго обучаться. Поэтому за инициализацию топологии отвечают специальные генераторы, если необходимо ограничить
или расширить разброс коэфициентов. В библиотеке два типа генераторов: первые для генерации чисел с плавающей точкой - используется
для инициализации топологии, второй для генерации целых чисел - обычно применяется в паре со стохастическим градиентным спуском.
Не стоит забывать, что нейросети представляются в виде слоев, а каждый слов имеет по свои внутренные компоненты. Часто используемые:
вектора и матрицы, но так же присутствует тензор - это двухмерная матрица, или трёхмерный вектор. Отметим, что данные понятия
взаимозаменяемые не только словесно но и чисто технически. Любой вектор можно представить как тензор, а любую матрицу как вектор.
Но и на этом всё не заканчивается - абсолютно любой вектор/матрицу/тензор можно представить в виде Range (диапазона). Такой тип
служит для безболезненного внедрения common типов для их хранения в контейнерах и предачи функциям общей сигнатуры.
Столь забавную архитектуру вынесено в отдельную библиотеку, которая композитивно пренаджелит обсуждаемой в целом. В кратце в ней
присутсвуют не только тенорные классы, но и различные алгоритмы, написанные для обучения нейросети с подкреплением.
Все что перечисленно в этом разделе присутствует в библиотеке в полной мере и может быть кастомизировано.

Компоненты для обучения, сериализации, тестирования нейросети:
Модульная архитектура дает четкое понимание того, что ни один класс не делает больше, чем от него просят. Нейросети ничего не умеют,
кроме как получить данные и выдать результат. Поэтому как не сложно догадаться для их обучения используются специальные классы
Training. Они принимают тип нейросети, и на основе типа знают как обучать нейросеть, а также предоставляют стандартный набор
алгоритмов обучения.
Сериализация позволит пользоваться уже обученными нейросетями не тратя время на обучение с нуля. Никто не запрещает разбить цикл
обучения на несколько итераций, пользователь всегда может продолжить обучени когда захочет. Хоть сериализация и бинарная - что
чревато неперосимостью, при сериализации первые несколько байт используются для кодирования мета данных о размере типов.
Это дает возможность сериализаровать нейросеть на одной архитектуре и десериализировать на другой. Вся магия заключается в буфере,
в который сначала записываются данные в воответствии с размером, а потом уже с этого буфера данные считываются, делая нужные
переобразования типов.
Обучать нейросеть можно сколько угодно, нейросеть не знает хорошо ли она справляется с поставленой задачей. Следовательно нам
необходимо прверить качество её обучения. Тренер может также давать показания состояния нейросети, но с этой задачей лучше
справляются классы Accuracy и Checker. Accuracy имеет несколько режимов проверки - эти показания дают представления о том,
насколько локально или глобально предсказывает ответ нейросеть. Checker предоставляет информацию об отклонении желаемого
результата от реального. В целом довольно трудно заставить крупную нейросеть не ошибаться вовсе. Но если она никогда не ошибается
значит она переобучена. Это тоже плохо. Поэтому по этому коефициенту можно дать обьективную оценку внутреннего состояния нейросети.

Линейная и Полиномиальная регрессии:
Помимо нейросети можно так же довольствоваться регрессией данных. В библиотеке два типа регрессии: линейная для многомерного случая
и полиномиальная для одномерного. Почему нет полиномиальной регрессии для многомерного случая? Потому что, функция такой сложности
будет равнятся нейросети. Типы регрессии подчиняются тем же правилам, что и нейросети. За исключением того, что процесс обучения
происходит моментально, а заполнять случайными значениями топологию нет необходимости.

Резюме:
Данной библиотекой была обучена полносвязная нейросеть с точностью 0.998% на тренеровочном и 0.981% на тестовом пакетах, умеющая
распознавать рукописные цифры на картинке 28x28 пикселей с одним каналом цвветов, тоесть только чёрнобелая картинка.
Хоть и полносвязная нейросеть не продеставляется такой большой гибкости - она является основой основ среди класических нейросетей.
Заботливая библиотека предоставляет возможность для создания нейросети с разновидными слоями, а благодаря расширяемости,
не составит труда написать новые.

Ссылка на github: github.com/Sigma-Ryden/TrixyNet
